{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import string\n",
    "import itertools\n",
    "import re\n",
    "from nltk import WordNetLemmatizer, word_tokenize,sent_tokenize,  RegexpTokenizer\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "from nltk import FreqDist\n",
    "lemmatize = WordNetLemmatizer()\n",
    "unknown_token = \"UNKNOWN_TOKEN\"\n",
    "pad_token = \"PAD_TOKEN\"\n",
    "tweetknzr = TweetTokenizer()\n",
    "max_sentance_length = 100\n",
    "\n",
    "def clean_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    clean_tokens = [lemmatize.lemmatize(token.lower().strip()) for token in tokens]\n",
    "    return ' '.join(clean_tokens)\n",
    "def clean_sentance(text):\n",
    "    sentences = sent_tokenize(data.lower())\n",
    "    sentences = [tweetknzr.tokenize(sent) for sent in sentences]\n",
    "    return sentences\n",
    "\n",
    "def pad_sentances(sentance):\n",
    "    padded = []\n",
    "    sentance = word_tokenize(sentance)\n",
    "    \n",
    "    pad_len = max_sentance_length - len(sentance)\n",
    "    if pad_len > 0:\n",
    "        for _ in range(pad_len):\n",
    "            padded.append(pad_token)\n",
    "        \n",
    "    for word in sentance:\n",
    "        padded.append(word)   \n",
    "    return padded\n",
    "\n",
    "def flatten_comments(comments):\n",
    "    flattened = []\n",
    "    for comment in comments:\n",
    "        for sentance in sent_tokenize(comment):\n",
    "            flattened.append(sentance)\n",
    "    return flattened\n",
    "\n",
    "def truncate_sentances(sentance):\n",
    "    if len(sentance) > max_sentance_length:\n",
    "        sentance = sentance[:max_sentance_length]\n",
    "    return sentance\n",
    "\n",
    "## make main in a second\n",
    "with open(\"output_comments_query_crypto_all.txt\") as f:\n",
    "    data = f.read()\n",
    "    data = data.replace('\\n','')\n",
    "\n",
    "\n",
    "raw_sentances = clean_sentance(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_less_spammy(raw_sentances):\n",
    "    allowed = []\n",
    "    spam_chars = ['/','(',')',':',';']\n",
    "    for sentance in raw_sentances:\n",
    "        tmp = []\n",
    "        for char in ' '.join(sentance):\n",
    "            if char not in spam_chars:\n",
    "                tmp.append(char)\n",
    "\n",
    "        text = ''.join(tmp)\n",
    "        text = re.sub(r\"http\\S+\", \"\", text)\n",
    "        text = re.sub(r' ,', ',', text)\n",
    "        text = re.sub(r'-', ' ', text)\n",
    "        text = re.sub(r'\\(', '', text)\n",
    "        text = re.sub(r'\\[', '', text)\n",
    "        text = re.sub(r' r ', '', text) \n",
    "        text = re.sub(r\" ’ \", \"’\", text) \n",
    "        text = re.sub(r'$ ', '$', text) \n",
    "        text = re.sub(r' k ', 'k', text)\n",
    "        text = re.sub(r'>', '', text)\n",
    "        text = re.sub(r'\\$ ', '$', text)\n",
    "        text = re.sub(r' % ', '% ', text)\n",
    "        text = re.sub(r'\\s+([?.!\"])', r'\\1', text) \n",
    "        text = re.sub(']  ','',text)\n",
    "        text = re.sub(r'_','',text)\n",
    "        text = re.sub(r'^','',text)\n",
    "        text = re.sub(r'\\.(?! )', '. ', re.sub(r' +', ' ', text))\n",
    "        text = re.sub('\\.\\.\\.\\.?', '', text)  # remove ellipses)\n",
    "        text = re.sub(r'[^ A-Za-z0-9.,%$!?]', '', text)  # other punctuation\n",
    "        tmp = [x for x in text.split(' ') if x]\n",
    "        text = ' '.join(tmp)\n",
    "        allowed.append(text)\n",
    "    return allowed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allowed = make_less_spammy(raw_sentances)\n",
    "flattend = flatten_comments(allowed)\n",
    "\n",
    "truncated = [truncate_sentances(sent) for sent in flattend]\n",
    "lengths = [len(sent) for sent in truncated]\n",
    "max(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 1 µs, total: 5 µs\n",
      "Wall time: 8.11 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "padded = [pad_sentances(sent) for sent in truncated]\n",
    "word_tokenized_sentences = padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = [len(sent) for sent in word_tokenized_sentences]\n",
    "min(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28668 unique words tokens.\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 25000\n",
    "word_freq = FreqDist(itertools.chain(*word_tokenized_sentences))\n",
    "print(\"Found %d unique words tokens.\" % len(word_freq.items()))\n",
    "vocab = word_freq.most_common(len(word_tokenized_sentences)-1)\n",
    "index_to_word = [x[0] for x in vocab]\n",
    "index_to_word.append(unknown_token)\n",
    "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
    "\n",
    "\n",
    "# Replace all words not in our vocabulary with the unknown token\n",
    "for i, sent in enumerate(word_tokenized_sentences):\n",
    "    word_tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8898420, 65601000)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.Word2Vec(word_tokenized_sentences, size=100, window=5, min_count=1, workers=2,sg=1)\n",
    "model.train(word_tokenized_sentences, total_examples=len(word_tokenized_sentences),epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mcassettix/anaconda3/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('misquoting', 0.7109144330024719),\n",
       " ('liars', 0.7064816951751709),\n",
       " ('buterin', 0.638586699962616),\n",
       " ('csw', 0.62827068567276),\n",
       " ('taylor', 0.6084359288215637),\n",
       " ('bcash', 0.6077585220336914),\n",
       " ('vlad', 0.6024558544158936),\n",
       " ('consensys', 0.5981085896492004),\n",
       " ('cannon', 0.5963471531867981),\n",
       " ('mlm', 0.5903977751731873)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('vitalik' ,topn=10) #creator of ethereum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('mymodel_tagged') ## this isn't used for exploratory analysis, it's used for tagging\n",
    "import pickle\n",
    "with open('cleaned_sentances_tagged_padded.pkl','wb') as fp:\n",
    "    pickle.dump(word_tokenized_sentences,fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
